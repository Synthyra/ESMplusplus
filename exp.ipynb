{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4030016967c4454aa8d3149cbab5ec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Logan\\.cache\\huggingface\\hub\\models--Synthyra--ESMplusplus_small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40965a38947e4275b9200ce0848ff8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_esm_plusplus.py:   0%|          | 0.00/32.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Synthyra/ESMplusplus_small:\n",
      "- modeling_esm_plusplus.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f429abef24e3460b914d8c1ef9b86dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Logan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ESMplusplusForSequenceClassification were not initialized from the model checkpoint at Synthyra/ESMplusplus_small and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.2.bias', 'classifier.2.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Synthyra/ESMplusplus_small\", trust_remote_code=True, num_labels=4)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMplusplusForSequenceClassification(\n",
       "  (embed): Embedding(64, 960)\n",
       "  (transformer): TransformerStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-29): 30 x UnifiedTransformerBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (layernorm_qkv): Sequential(\n",
       "            (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=960, out_features=2880, bias=False)\n",
       "          )\n",
       "          (out_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (q_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (k_ln): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (rotary): RotaryEmbedding()\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=960, out_features=5120, bias=False)\n",
       "          (2): SwiGLU()\n",
       "          (3): Linear(in_features=2560, out_features=960, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (sequence_head): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=960, out_features=64, bias=True)\n",
       "  )\n",
       "  (ce_loss): CrossEntropyLoss()\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1920, out_features=3840, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((3840,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=3840, out_features=4, bias=True)\n",
       "  )\n",
       "  (mse): MSELoss()\n",
       "  (ce): CrossEntropyLoss()\n",
       "  (bce): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "ESMplusplusOutput(loss=tensor(32., grad_fn=<NllLossBackward0>), logits=tensor([[64.,  0.,  0.,  0.],\n",
      "        [64.,  0.,  0.,  0.]], grad_fn=<AddmmBackward0>), last_hidden_state=tensor([[[-0.0015, -0.0021,  0.0062,  ...,  0.0058, -0.0023, -0.0101],\n",
      "         [-0.0222, -0.0142,  0.0519,  ...,  0.0017,  0.0189, -0.0007],\n",
      "         [-0.0461, -0.0351,  0.0073,  ...,  0.0291,  0.0173, -0.0182],\n",
      "         ...,\n",
      "         [-0.0224,  0.0053,  0.0110,  ...,  0.0105, -0.0066, -0.0260],\n",
      "         [-0.0209, -0.0269,  0.0143,  ...,  0.0138,  0.0152, -0.0154],\n",
      "         [-0.0305, -0.0258,  0.0168,  ...,  0.0107,  0.0223, -0.0148]],\n",
      "\n",
      "        [[ 0.0021, -0.0066,  0.0084,  ...,  0.0074, -0.0047, -0.0088],\n",
      "         [-0.0192, -0.0184,  0.0412,  ...,  0.0367,  0.0079,  0.0022],\n",
      "         [-0.0296, -0.0512,  0.0186,  ...,  0.0244, -0.0138,  0.0069],\n",
      "         ...,\n",
      "         [-0.0417, -0.0303,  0.0023,  ...,  0.0016, -0.0020, -0.0031],\n",
      "         [ 0.0071, -0.0096, -0.0061,  ...,  0.0206, -0.0221, -0.0289],\n",
      "         [-0.0129, -0.0130,  0.0148,  ...,  0.0025, -0.0064, -0.0149]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=())\n"
     ]
    }
   ],
   "source": [
    "sequences = ['MPRTEIN', 'MSEQWENCE']\n",
    "tokenized = tokenizer(sequences, padding=True, return_tensors='pt')\n",
    "\n",
    "labels = torch.randint(0, 4, (2,)).long() # random labels for the two sequences\n",
    "tokenized['labels'] = labels\n",
    "# if you are on device, make sure to move the tokenized to the device\n",
    "# tokenized = {k: v.to(device) for k, v in tokenized.items()}\n",
    "output = model(**tokenized)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
